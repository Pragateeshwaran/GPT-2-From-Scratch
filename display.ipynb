{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from main import GPT2, GPTConfig\n",
    "from transformers import GPT2Tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2(GPTConfig())\n",
    "model.load_state_dict(torch.load('epochs_25141.pth'))\n",
    "model.to('cuda')\n",
    "enc = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "sample 0: \n",
      "Hello I am pragateesh with my own idea, as it appears in the next three sentences\n",
      "\n",
      "There is a question where many, even many a people\n",
      "\n",
      "\n",
      "sample 1: \n",
      "Hello I am pragateesh, the only person in the world with a free spirit. I'm sorry I am so strong, but my powers are not yours\n",
      "\n",
      "\n",
      "sample 2: \n",
      "Hello I am pragateesh, but I don't want to do my own training at this level, but I would like to spend my life studying in the\n",
      "\n",
      "\n",
      "sample 3: \n",
      "Hello I am pragateesh; it's an honest way of living. If you can do something very simple with something trivial to your liking, then I can\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_return_sequences = 4\n",
    "max_length = 32\n",
    "tokens = enc.encode(\"Hello I am pragateesh\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "xgen = tokens.to(device)\n",
    "sample_rng = torch.Generator(device=device)\n",
    "while xgen.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, _ = model(xgen)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng)\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\"\\nsample {i}: \\n{decoded}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
