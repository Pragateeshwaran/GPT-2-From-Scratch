{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from main import GPT2, GPTConfig\n",
    "from transformers import GPT2Tokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2(GPTConfig())\n",
    "model.load_state_dict(torch.load('epochs_25141.pth'))\n",
    "model.to('cuda')\n",
    "enc = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "sample 0: \n",
      "Hello I am pragateeshark. My name was on that board!\" This is a very similar approach of the first place game I made. This was a\n",
      "\n",
      "\n",
      "sample 1: \n",
      "Hello I am pragateeshut!\"\n",
      "\n",
      "I thought that by now it appeared that I was at an end of my rope: \"Hehehehe\n",
      "\n",
      "\n",
      "sample 2: \n",
      "Hello I am pragateesh. I always said not to care how much they look like I was talking to you, but I am pragateesh. I\n",
      "\n",
      "\n",
      "sample 3: \n",
      "Hello I am pragateesh. When I see a guy with a beard all the time, I say \"Omg what's he doing here\", and I\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "num_return_sequences = 4\n",
    "max_length = 32\n",
    "tokens = enc.encode(\"Hello I am pragateesh\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
    "xgen = tokens.to(device)\n",
    "sample_rng = torch.Generator(device=device)\n",
    "sample_rng.manual_seed(42)\n",
    "while xgen.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        # with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
    "        logits, _ = model(xgen)\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, 1, generator=sample_rng)\n",
    "        xcol = torch.gather(topk_indices, -1, ix)\n",
    "        xgen = torch.cat((xgen, xcol), dim=1)\n",
    "for i in range(num_return_sequences):\n",
    "    tokens = xgen[i, :max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(f\"\\nsample {i}: \\n{decoded}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
